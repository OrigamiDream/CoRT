program: run_pretraining.py
project: CoRT Pre-training
description: Bayes Search for CoRT Pre-training Hyperparameters
method: bayes
metric:
  name: val_loss
  goal: minimize
parameters:
  repr_size:
    value: 1024
  repr_act:
    values:
      - none
      - tanh
      - swish
      - gelu
  loss_base:
    value: supervised
  backbone_trainable_layers:
    value: -1  # unfreeze all backbone model parameters
  lr_fn:
    value: polynomial_decay
  warmup_rate:
    value: 0.01  # total 100 steps
  num_train_steps:
    value: 10000  # total 10,000 steps without epochs
  model_name:
    values:
      - klue/roberta-base
      - korscibert
      - korscielectra
  learning_rate:
    distribution: uniform
    min: 0.00001
    max: 0.0001
  layerwise_lr_decay:
    distribution: uniform
    min: 0.6
    max: 1.0
  weight_decay:
    distribution: uniform
    min: 0.0
    max: 0.01
  batch_size:
    values:
      - 64
      - 128