program: run_finetuning.py
project: CoRT Fine-tuning
description: Grid Search for CoRT Fine-tuning Hyperparameters from Pre-trained backbone
method: grid
metric:
  name: val_accuracy
  goal: maximize
parameters:
  restore_checkpoint:
    value: latest
  pretraining_run_name:  # klue/roberta-base W&B run names
    values: [zwyafomh, u7rbr3je, ns4bedao, o1vzqxdn, wcxjgkdr, 2fzgjur4, myrzu7in, 4py7flt5, l4b2e95z, 6cnr8gb9, xzxa97jz, pyl5nczg, 5b6lcuno, lo4f6pma, ujh5yse9, qiha6oci, ylaayw7v, 6b2dulbe, k99rat0p, i9pfyw3i, u881sk6f, ezahqzxs, 5y7kkg5c, h89oi3o7]
  batch_size:
    value: 64
  repr_size:
    value: 1024
  repr_classifier:
    values:
      - seq_cls
      - bi_lstm
  repr_act:
    values:
      - none
      - tanh
      - swish
      - gelu
  repr_preact:
    values:
      - True
      - False
  loss_base:
    values:
      - margin
      - supervised
  backbone_trainable_layers:
    value: 0  # freeze all backbone model parameters
  lr_fn:
    value: polynomial_decay
  warmup_rate:
    value: 0.06
  epochs:
    value: 10
  model_name:
    value: klue/roberta-base
  learning_rate:
    value: 5e-5
  weight_decay:
    value: 0
  alpha:
    value: 2.0
  concat_hidden_states:
    value: 1